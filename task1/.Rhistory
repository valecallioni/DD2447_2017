D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eig = eigen(L)
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), sort(eigen(D-A)$values), ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eig$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
par(mfrow=c(1,2))
plot(1:length(eig$values), sort(eigen(D-A)$values), ylab = "Eigenvalues of Laplacian")
plot(1:length(eig$values), sort(eig$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
image(1:dim(A)[1],1:dim(A)[1],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
N = 500
phi = 0.98
sigma = 0.16
sigma^2/(1-phi^2)
?array
T = 500
N = 1000
phi = 0.98
sigma = 0.16
wss = array(0,c(T,N))
View(wss)
dim(wss)
?rnorm
x0 = rnorm(N,0,sqrt(sigma^2/(1-phi^2)))
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N,0,sqrt(sigma^2/(1-phi^2)))
x[,1] = rnorm(N,phi*x0,sigma)
xss[,1] = rnorm(N,phi*x0,sigma)
beta = 0.5
y = c(2)
xss = array(0,c(T,N))
wss = array(0,c(T,N))
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[,1] = rnorm(N, phi*x0, sigma)
wss[,1] = pnorm(y[1], 0, (beta^2)*exp(xss[,1]))
View(wss)
T = 500
N = 100
phi = 0.98
sigma = 0.16
beta = 0.5
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
length(rnorm(N, phi*x0, sigma))
length(pnorm(y[1], 0, (beta^2)*exp(xss[1,])) )
length(rnorm(1, 0, (beta^2)*exp(xss[1,])))
length(exp(xss[1,])
)
1:2:0.1
1:0.1:2
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# wss[1,] = pnorm(y[1], 0, (beta^2)*exp(xss[1,]))
for (i in 1:N)
wss[1,i] = rnorm(1, 0, (beta^2)*exp(xss[1,i]))
for (t in 2:T){
xss[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
wss[t,i] = rnorm(1, 0, (beta^2)*exp(xss[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(wss[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
?boxplot
?boxplot.matrix
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood, use.cols=FALSE)
boxplot.matrix(likelihood)
####################################################################
# SEQUENTIAL IMPORTANCE SAMPLING
####################################################################
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
x[1,i] = rnorm(1, phi*x0[i], sigma)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
for (i in 1:N){
x[t,i] = rnorm(1, phi*x[t-1,i], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood)
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
# The plot of the eigenvalues of the symmetric normalized Laplacian is used to confirm
# the conclusions drawn by the other plot.
# Also, to identify k clusters,  the eigenvalue spectrum of L  must have a gap,
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eigen(L)$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
# If good clusters can be identified, then the  Laplacian L  is approximately block-diagonal,
# with each block defining a cluster.
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
# This function actually computes the clusters through the transformation of the data
spectralClustering = function(eigenvectors, k){
Y = eigenvectors[,1:k]
clustering = kmeans(Y, centers = k)
# Each row of the original matrix A (each node) is assigned to a cluster
# clustering$cluster is a vector of n elements, each element correspond to a node
return(clustering$cluster)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
V2 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example2.dat")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
y = read.table("output.txt")
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
max_value = NULL
for (i in 1:10){
max_value = c(max_value, which(likelihood[i,]==max(likelihood[i,])))
}
max_value
i = 1
max(likelihood[i,]
)
View(likelihood)
View(likelihood)
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
View(likelihood)
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
prova = likelihood[,1]
prova[-which(is.nan)]
prova[-which(is.nan(prova))]
prova=prova[-which(is.nan(prova))]
max_value = NULL
for (i in 1:10){
temp = likelihood[i,]
temp = temp[-which(is.nan(temp))]
max_value = c(max_value, which(temp==max(temp)))
}
max_value
beta[2]
beta[3]
beta = seq(0.05, 1, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
max_value = NULL
for (i in 1:10){
temp = likelihood[i,]
temp = temp[-which(is.nan(temp))]
max_value = c(max_value, which(temp==max(temp)))
}
max_value
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
max_value = NULL
for (i in 1:10){
temp = likelihood[i,]
temp = temp[-which(is.nan(temp))]
max_value = c(max_value, which(temp==max(temp)))
}
max_value
M = NULL
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(temp))]
M = c(M, mean(m))
}
M
M = NULL
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
M = c(M, mean(m))
}
M
i=1
m = likelihood[,i]
m = m[-which(is.nan(m))]
M = c(M, mean(m))
M = NULL
m = likelihood[,i]
m = m[-which(is.nan(m))]
M = c(M, mean(m))
M = NULL
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M = c(M, mean(m))
M = NULL
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M = c(M, mean(m))
}
M
M = array(0,20)
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
M
M = array(-1000,20)
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
M
idx = which(M==max(M))
beta[idx]
y = read.table("output.txt")
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihoodResampl, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC with resampling", names = round(beta, digit=2))
M = array(-1000,20)
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
M
idx = which(M==max(M))
beta[idx]
M = array(0,20)
for (i in 1:20){
m = likelihood[,i]
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
M
View(likelihood)
i=1
likelihood[,i]
which(is.nan(m))
m = likelihood[,i]
m = m[-which(is.nan(m))]
M = array(0,20)
for (i in 1:20){
m = likelihood[,i]
if (which(is.nan(m))>0)
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
i=1
m = likelihood[,i]
if (which(is.nan(m))>0)
m = m[-which(is.nan(m))]
which(is.nan(m))>0
which(is.nan(m))!=logical(0)
as.vector(logical(0))
tyoe(logical(0))
type(logical(0))
?logical(0)
M = array(0,20)
for (i in 1:20){
m = likelihood[,i]
if (which(is.nan(m)))
m = m[-which(is.nan(m))]
if (length(m)>0)
M[i] = mean(m)
}
as.numeric(logical(0))
as.numeric(logical(0))==0
M = array(0,20)
for (i in 1:20){
m = likelihood[,i]
if (length(m)>0)
M[i] = mean(m)
}
M
idx = which(M==max(M))
beta[idx]
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
y = read.table("output.txt")
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
beta = seq(0.05, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihoodResampl, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC with resampling", names = round(beta, digit=2))
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihood, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihoodResampl, y=y)
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC with resampling", names = round(beta, digit=2))
