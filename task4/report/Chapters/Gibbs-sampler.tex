% Chapter Template

\chapter{Gibbs Sampler} % Main chapter title

\label{Gibbs} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{3. \emph{Gibbs Sampler}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
We use a Gibbs sampler to estimate the posterior $p(r_1, \dots, r_N | D)$ where $D$ is the set of sequences $s^1, \dots, s^N$ generated by the magic word model and $r_n$ is the start position of the magic word in the $n$:th sequence $s^n$.

We use a \textit{collapsed} Gibbs sampler, which means collapse out the Dirichlet distributions(the prior distributions over the categorical variables). The result of this collapsing introduces dependencies among all the categorical variables dependent on a given Dirichlet prior.

Each state $R^{(s)}$ of a the Markov chain has the following form : $R^{(s)} = r_1^{(s)} \dots r_N^{(s)}$.

The first $T$ samples (burn-in period) are discarded to allow for the Markov chain to reach stationarity. We also collect subsequent samples after a lag to ensure their correlation is low.

The implementation is as follows: 

\begin{algorithmic}[1]
\State Random initialization $R^{(0)}$
\For{i from 0 to iterations}
    \For{n from 0 to N}
            \State $P(r^{(i)}_{n}|R_{-n}^{(i)}, D) \propto \cup_{l = 0}^{M-W-1} p(D_{background}| R^{(i)},\alpha') \prod\limits_{j = l}^{l + W} p(D_{j}|R^{(i)},\alpha)$
            \State normalize the above vector of probabilities
        \State $r^{(i)}_{n} \sim Cat(P(r^{(i)}_{n}|R_{-n}^{(i)}, D))$ \Comment $r^{(i)}_{n}$ is sampled from the categorical with parameter $P(r^{(i)}_{n}|R_{-n}^{(i)}, D)$, which is the vector of posterior probabilities for values of $r^{(i)}_{n}$ in $[0, M-W-1]$
    \EndFor
\EndFor

\For{n from 0 to N}
    \State samples = samples[i] \Comment samples is the vector containing the Markov chain states (samples) that were collected
    \State $r_n = argmax(count_{t = T:k:iterations}(R^{(t)}_{n}))$ \Comment For each sequence, the starting position is the mode of the posterior, i.e. the position that was sampled most frequently . The $T$ first samples are discarded and we only consider every $k$:th iteration in the samples, where $k > 1$.
\EndFor
\end{algorithmic}

With : 

\begin{equation}
p(D_{background}|R^{(i)}, \alpha') = \frac{\Gamma(\sum_k \alpha'_k)}{\Gamma(B \sum_k \alpha'_k)} \prod_k \frac{\Gamma(B_k + \alpha'_k)}{\Gamma(\alpha'_k)}
\end{equation}
where $B = N(M-W))$ is the number of background positions and $B_k$ is the count of symbol $k$ in the background.
\begin{equation}
p(D_{j}|R^{(i)},\alpha) = \frac{\Gamma(\sum_k \alpha_k)}{\Gamma(N \sum_k \alpha_k)} \prod_k \frac{\Gamma(N^j_k + \alpha_k)}{\Gamma(\alpha_k)}
\end{equation}
where $N^j_k$ is the count of symbol $k$ in the $j$:th column of the magic word.
