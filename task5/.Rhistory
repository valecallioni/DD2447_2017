invD = matrix(0, nrow = dim(A)[1], ncol = dim(A)[2])
for (i in 1:dim(A)[1]){
invD[i,i] = 1/sum(A[i,])
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eig = eigen(L)
eig$values
invD
sqrt(invD)
for (i in 1:dim(A)[1]){
invD[i,i] = 1/sum(A[i,])
}
L
?eigen
eig = eigen(L, symmetric = TRUE)
eig$values
col1 = c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6)
col2 = c(2, 3, 1, 3, 4, 1, 2, 2, 5, 6, 4, 6, 4, 5)
A = matrix(0, nrow = 6, ncol = 6)
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
A
L = diag(dim(A)[1]) - sqrt(invD)%*%A%*%sqrt(invD)
eig = eigen(L, symmetric = TRUE)
eig$values
plot(1:dim(A)[1], eig$values)
col1 = c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6)
col2 = c(2, 3, 1, 3, 4, 1, 2, 2, 5, 6, 4, 6, 4, 5)
A = matrix(0, nrow = 6, ncol = 6)
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eig = eigen(L, symmetric = TRUE)
plot(1:dim(A)[1], eigen(D-A)$values)
eigen(D-A)$values
col1 = c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6)
col2 = c(2, 3, 1, 3, 4, 1, 2, 2, 5, 6, 4, 6, 4, 5)
A = matrix(0, nrow = 6, ncol = 6)
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
eigen(diag(dim(A)[1])-D+A)$values
L = sqrt(invD)%*%A%*%sqrt(invD)
eigen(L)$values
# This function helps us to visualize the Sorted Fiedler Vector
# This allows us to detect the number of clusters to select
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = matrix(0, nrow = dim(A)[1], ncol = dim(A)[2])
for (i in 1:dim(A)[1]){
invD[i,i] = 1/sum(A[i,])
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
x11()
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Lapiacian")
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(D-A), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
# This function actually computes the clusters through the transformation of the data
spectralClustering = function(eigenvectors, k){
Y = eigenvectors[,1:k]
clustering = kmeans(Y, centers = k)
# Each row of the original matrix A (each node) is assigned to a cluster
# clustering$cluster is a vector of n elements, each element correspond to a node
return(clustering$cluster)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
# This function helps us to visualize the Sorted Fiedler Vector
# This allows us to detect the number of clusters to select
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
x11()
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Lapiacian")
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(D-A), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
V2 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example2.dat")
?par
# This function helps us to visualize the Sorted Fiedler Vector
# This allows us to detect the number of clusters to select
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
x11()
par(frow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), eigen(L)$values, ylab = "Eigenvalues of symmetric normalized Laplacian")
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
spectralClustering = function(eigenvectors, k){
Y = eigenvectors[,1:k]
clustering = kmeans(Y, centers = k)
# Each row of the original matrix A (each node) is assigned to a cluster
# clustering$cluster is a vector of n elements, each element correspond to a node
return(clustering$cluster)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
# This function helps us to visualize the Sorted Fiedler Vector
# This allows us to detect the number of clusters to select
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
x11()
par(frow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eigen(L)$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
# This function helps us to visualize the Sorted Fiedler Vector
# This allows us to detect the number of clusters to select
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eigen(L)$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
V2 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example2.dat")
col1 = c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6)
col2 = c(2, 3, 1, 3, 4, 1, 2, 2, 5, 6, 4, 6, 4, 5)
A = matrix(0, nrow = 6, ncol = 6)
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eig = eigen(L)
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), sort(eigen(D-A)$values), ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eig$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
par(mfrow=c(1,2))
plot(1:length(eig$values), sort(eigen(D-A)$values), ylab = "Eigenvalues of Laplacian")
plot(1:length(eig$values), sort(eig$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
image(1:dim(A)[1],1:dim(A)[1],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
N = 500
phi = 0.98
sigma = 0.16
sigma^2/(1-phi^2)
?array
T = 500
N = 1000
phi = 0.98
sigma = 0.16
wss = array(0,c(T,N))
View(wss)
dim(wss)
?rnorm
x0 = rnorm(N,0,sqrt(sigma^2/(1-phi^2)))
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N,0,sqrt(sigma^2/(1-phi^2)))
x[,1] = rnorm(N,phi*x0,sigma)
xss[,1] = rnorm(N,phi*x0,sigma)
beta = 0.5
y = c(2)
xss = array(0,c(T,N))
wss = array(0,c(T,N))
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[,1] = rnorm(N, phi*x0, sigma)
wss[,1] = pnorm(y[1], 0, (beta^2)*exp(xss[,1]))
View(wss)
T = 500
N = 100
phi = 0.98
sigma = 0.16
beta = 0.5
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
length(rnorm(N, phi*x0, sigma))
length(pnorm(y[1], 0, (beta^2)*exp(xss[1,])) )
length(rnorm(1, 0, (beta^2)*exp(xss[1,])))
length(exp(xss[1,])
)
1:2:0.1
1:0.1:2
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# wss[1,] = pnorm(y[1], 0, (beta^2)*exp(xss[1,]))
for (i in 1:N)
wss[1,i] = rnorm(1, 0, (beta^2)*exp(xss[1,i]))
for (t in 2:T){
xss[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
wss[t,i] = rnorm(1, 0, (beta^2)*exp(xss[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(wss[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
?boxplot
?boxplot.matrix
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood, use.cols=FALSE)
boxplot.matrix(likelihood)
####################################################################
# SEQUENTIAL IMPORTANCE SAMPLING
####################################################################
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
x[1,i] = rnorm(1, phi*x0[i], sigma)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
for (i in 1:N){
x[t,i] = rnorm(1, phi*x[t-1,i], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood)
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
# The plot of the eigenvalues of the symmetric normalized Laplacian is used to confirm
# the conclusions drawn by the other plot.
# Also, to identify k clusters,  the eigenvalue spectrum of L  must have a gap,
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eigen(L)$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
# If good clusters can be identified, then the  Laplacian L  is approximately block-diagonal,
# with each block defining a cluster.
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
# This function actually computes the clusters through the transformation of the data
spectralClustering = function(eigenvectors, k){
Y = eigenvectors[,1:k]
clustering = kmeans(Y, centers = k)
# Each row of the original matrix A (each node) is assigned to a cluster
# clustering$cluster is a vector of n elements, each element correspond to a node
return(clustering$cluster)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
V2 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example2.dat")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task5")
source("task5_functions.R")
library(invgamma)
library(pscl)
T = 500
N = 1000
phi = 0.98
a = 0.01
b = 0.01
X = NULL        # will have dimension TXN
sigma2 = NULL   # will have dimension 1xN
beta2 = NULL    # will have dimension 1xN
y = read.table("output.txt")
sigma0 = rigamma(1, a, b)
